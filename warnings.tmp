Enter your choice (1-6): Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
24/07/16 15:32:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Enter zip code (for example: 01824)Enter Month (1-12): Enter Year (for example: 2024): [Stage 0:>                                                          (0 + 1) / 1]24/07/16 15:32:36 ERROR ShuffleBlockFetcherIterator: Error occurred while fetching local blocks
java.nio.file.NoSuchFileException: C:\Users\Learner_9ZH3Z134\AppData\Local\Temp\blockmgr-5a29d6c0-53e6-41b2-9772-0fbcb9e279f4\30\shuffle_0_0_0.index
	at java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:85)
	at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)
	at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:108)
	at java.base/sun.nio.fs.WindowsFileSystemProvider.newByteChannel(WindowsFileSystemProvider.java:234)
	at java.base/java.nio.file.Files.newByteChannel(Files.java:379)
	at java.base/java.nio.file.Files.newByteChannel(Files.java:431)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockData(IndexShuffleBlockResolver.scala:586)
	at org.apache.spark.storage.BlockManager.getLocalBlockData(BlockManager.scala:698)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocks(ShuffleBlockFetcherIterator.scala:574)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:723)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:194)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:73)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:232)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1570)
24/07/16 15:32:36 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\Learner_9ZH3Z134\AppData\Local\Temp\spark-4733c55b-f2ec-4c9e-9c62-d750c03385a0\pyspark-82275c19-465e-4056-bfda-f9320dbbf573
java.nio.file.NoSuchFileException: C:\Users\Learner_9ZH3Z134\AppData\Local\Temp\spark-4733c55b-f2ec-4c9e-9c62-d750c03385a0\pyspark-82275c19-465e-4056-bfda-f9320dbbf573
	at java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:85)
	at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)
	at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:108)
	at java.base/sun.nio.fs.WindowsFileAttributeViews$Basic.readAttributes(WindowsFileAttributeViews.java:53)
	at java.base/sun.nio.fs.WindowsFileAttributeViews$Basic.readAttributes(WindowsFileAttributeViews.java:38)
	at java.base/sun.nio.fs.WindowsFileSystemProvider.readAttributes(WindowsFileSystemProvider.java:197)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1858)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:124)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:117)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:90)
	at org.apache.spark.util.SparkFileUtils.deleteRecursively(SparkFileUtils.scala:121)
	at org.apache.spark.util.SparkFileUtils.deleteRecursively$(SparkFileUtils.scala:120)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1126)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1570)
Traceback (most recent call last):
  File "C:\Users\Learner_9ZH3Z134\AppData\Local\Programs\Python\Python311\Lib\site-packages\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Learner_9ZH3Z134\AppData\Local\Programs\Python\Python311\Lib\socket.py", line 706, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Learner_9ZH3Z134\AppData\Local\Programs\Python\Python311\Lib\site-packages\py4j\clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Learner_9ZH3Z134\AppData\Local\Programs\Python\Python311\Lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Learner_9ZH3Z134\AppData\Local\Programs\Python\Python311\Lib\site-packages\py4j\clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Capstone\scripts\Transaction_module.py", line 112, in <module>
    get_transaction_detail(spark)
  File "C:\Capstone\scripts\Transaction_module.py", line 97, in get_transaction_detail
    df.show(df.count(), truncate=False)
            ^^^^^^^^^^
  File "C:\Users\Learner_9ZH3Z134\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\sql\dataframe.py", line 1238, in count
    return int(self._jdf.count())
               ^^^^^^^^^^^^^^^^^
  File "C:\Users\Learner_9ZH3Z134\AppData\Local\Programs\Python\Python311\Lib\site-packages\py4j\java_gateway.py", line 1321, in __call__
    answer = self.gateway_client.send_command(command)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Learner_9ZH3Z134\AppData\Local\Programs\Python\Python311\Lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Learner_9ZH3Z134\AppData\Local\Programs\Python\Python311\Lib\site-packages\py4j\clientserver.py", line 538, in send_command
    logger.info("Error while receiving.", exc_info=True)
  File "C:\Users\Learner_9ZH3Z134\AppData\Local\Programs\Python\Python311\Lib\logging\__init__.py", line 1479, in info
    def info(self, msg, *args, **kwargs):

  File "C:\Users\Learner_9ZH3Z134\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\context.py", line 381, in signal_handler
    self.cancelAllJobs()
  File "C:\Users\Learner_9ZH3Z134\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\context.py", line 2446, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
    ^^^^^^^^^^^^^^
  File "C:\Users\Learner_9ZH3Z134\AppData\Local\Programs\Python\Python311\Lib\site-packages\py4j\java_gateway.py", line 1321, in __call__
    answer = self.gateway_client.send_command(command)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Learner_9ZH3Z134\AppData\Local\Programs\Python\Python311\Lib\site-packages\py4j\java_gateway.py", line 1053, in send_command
    response = self.send_command(command, binary=binary)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Learner_9ZH3Z134\AppData\Local\Programs\Python\Python311\Lib\site-packages\py4j\java_gateway.py", line 1036, in send_command
    connection = self._get_connection()
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Learner_9ZH3Z134\AppData\Local\Programs\Python\Python311\Lib\site-packages\py4j\clientserver.py", line 284, in _get_connection
    connection = self._create_new_connection()
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Learner_9ZH3Z134\AppData\Local\Programs\Python\Python311\Lib\site-packages\py4j\clientserver.py", line 291, in _create_new_connection
    connection.connect_to_java_server()
  File "C:\Users\Learner_9ZH3Z134\AppData\Local\Programs\Python\Python311\Lib\site-packages\py4j\clientserver.py", line 438, in connect_to_java_server
    self.socket.connect((self.java_address, self.java_port))
ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it
Traceback (most recent call last):
  File "C:\Capstone\scripts\main_console.py", line 68, in <module>
    main()
  File "C:\Capstone\scripts\main_console.py", line 53, in main
    view_zipcode_transactions()
  File "C:\Capstone\scripts\main_console.py", line 14, in view_zipcode_transactions
    subprocess.run(["python", "scripts/Transaction_module.py"])
  File "C:\Users\Learner_9ZH3Z134\AppData\Local\Programs\Python\Python311\Lib\subprocess.py", line 550, in run
    stdout, stderr = process.communicate(input, timeout=timeout)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Learner_9ZH3Z134\AppData\Local\Programs\Python\Python311\Lib\subprocess.py", line 1201, in communicate
    self.wait()
  File "C:\Users\Learner_9ZH3Z134\AppData\Local\Programs\Python\Python311\Lib\subprocess.py", line 1264, in wait
    return self._wait(timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Learner_9ZH3Z134\AppData\Local\Programs\Python\Python311\Lib\subprocess.py", line 1590, in _wait
    result = _winapi.WaitForSingleObject(self._handle,
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
^CERROR: The process "19656" not found.
